{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import datasets\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 7s 1us/step\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# prepare data\n",
    "mnist = datasets.mnist\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "x_train = (x_train.reshape(-1, 784) / 255).astype(np.float32)\n",
    "x_test = (x_test.reshape(-1, 784) / 255).astype(np.float32)\n",
    "t_train = np.eye(10)[t_train].astype(np.float32)\n",
    "t_test = np.eye(10)[t_test].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, activation='sigmoid'))\n",
    "# model.add(Dense(200, activation='sigmoid'))\n",
    "# model.add(Dense(200, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "600/600 - 5s - loss: 1.9092 - accuracy: 0.5806 - 5s/epoch - 8ms/step\n",
      "Epoch 2/30\n",
      "600/600 - 3s - loss: 1.3071 - accuracy: 0.7712 - 3s/epoch - 4ms/step\n",
      "Epoch 3/30\n",
      "600/600 - 2s - loss: 0.9560 - accuracy: 0.8189 - 2s/epoch - 4ms/step\n",
      "Epoch 4/30\n",
      "600/600 - 3s - loss: 0.7675 - accuracy: 0.8426 - 3s/epoch - 5ms/step\n",
      "Epoch 5/30\n",
      "600/600 - 3s - loss: 0.6581 - accuracy: 0.8550 - 3s/epoch - 5ms/step\n",
      "Epoch 6/30\n",
      "600/600 - 3s - loss: 0.5880 - accuracy: 0.8642 - 3s/epoch - 6ms/step\n",
      "Epoch 7/30\n",
      "600/600 - 5s - loss: 0.5395 - accuracy: 0.8703 - 5s/epoch - 8ms/step\n",
      "Epoch 8/30\n",
      "600/600 - 3s - loss: 0.5038 - accuracy: 0.8754 - 3s/epoch - 5ms/step\n",
      "Epoch 9/30\n",
      "600/600 - 3s - loss: 0.4765 - accuracy: 0.8791 - 3s/epoch - 4ms/step\n",
      "Epoch 10/30\n",
      "600/600 - 3s - loss: 0.4549 - accuracy: 0.8826 - 3s/epoch - 5ms/step\n",
      "Epoch 11/30\n",
      "600/600 - 4s - loss: 0.4373 - accuracy: 0.8857 - 4s/epoch - 6ms/step\n",
      "Epoch 12/30\n",
      "600/600 - 4s - loss: 0.4229 - accuracy: 0.8886 - 4s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "600/600 - 4s - loss: 0.4106 - accuracy: 0.8902 - 4s/epoch - 6ms/step\n",
      "Epoch 14/30\n",
      "600/600 - 3s - loss: 0.4001 - accuracy: 0.8917 - 3s/epoch - 5ms/step\n",
      "Epoch 15/30\n",
      "600/600 - 3s - loss: 0.3910 - accuracy: 0.8936 - 3s/epoch - 4ms/step\n",
      "Epoch 16/30\n",
      "600/600 - 3s - loss: 0.3830 - accuracy: 0.8952 - 3s/epoch - 4ms/step\n",
      "Epoch 17/30\n",
      "600/600 - 4s - loss: 0.3758 - accuracy: 0.8963 - 4s/epoch - 6ms/step\n",
      "Epoch 18/30\n",
      "600/600 - 4s - loss: 0.3695 - accuracy: 0.8976 - 4s/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "600/600 - 3s - loss: 0.3637 - accuracy: 0.8989 - 3s/epoch - 5ms/step\n",
      "Epoch 20/30\n",
      "600/600 - 3s - loss: 0.3585 - accuracy: 0.8999 - 3s/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "600/600 - 4s - loss: 0.3539 - accuracy: 0.9008 - 4s/epoch - 6ms/step\n",
      "Epoch 22/30\n",
      "600/600 - 4s - loss: 0.3494 - accuracy: 0.9017 - 4s/epoch - 6ms/step\n",
      "Epoch 23/30\n",
      "600/600 - 4s - loss: 0.3454 - accuracy: 0.9028 - 4s/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "600/600 - 3s - loss: 0.3416 - accuracy: 0.9030 - 3s/epoch - 5ms/step\n",
      "Epoch 25/30\n",
      "600/600 - 3s - loss: 0.3382 - accuracy: 0.9042 - 3s/epoch - 4ms/step\n",
      "Epoch 26/30\n",
      "600/600 - 3s - loss: 0.3349 - accuracy: 0.9048 - 3s/epoch - 5ms/step\n",
      "Epoch 27/30\n",
      "600/600 - 3s - loss: 0.3318 - accuracy: 0.9057 - 3s/epoch - 6ms/step\n",
      "Epoch 28/30\n",
      "600/600 - 4s - loss: 0.3289 - accuracy: 0.9063 - 4s/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "600/600 - 5s - loss: 0.3263 - accuracy: 0.9070 - 5s/epoch - 9ms/step\n",
      "Epoch 30/30\n",
      "600/600 - 5s - loss: 0.3236 - accuracy: 0.9082 - 5s/epoch - 9ms/step\n",
      "test_loss: 0.309, test_acc: 0.914\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, t_train,\n",
    "            epochs=30, batch_size=100,\n",
    "            verbose=2)\n",
    "\n",
    "loss, acc = model.evaluate(x_test, t_test, verbose=0)\n",
    "print('test_loss: {:.3f}, test_acc: {:.3f}'.format(\n",
    "    loss,\n",
    "    acc\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from keras import datasets\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(Model):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = Dense(hidden_dim, activation='sigmoid')\n",
    "        self.l2 = Dense(hidden_dim, activation='sigmoid')\n",
    "        self.l3 = Dense(hidden_dim, activation='sigmoid')\n",
    "        self.l4 = Dense(output_dim, activation='softmax')\n",
    "\n",
    "        self.ls = [self.l1, self.l2, self.l3, self.l4]\n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.ls:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# prepare data\n",
    "mnist = datasets.mnist\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "x_train = (x_train.reshape(-1, 784) / 255).astype(np.float32)\n",
    "x_test = (x_test.reshape(-1, 784) / 255).astype(np.float32)\n",
    "t_train = np.eye(10)[t_train].astype(np.float32)\n",
    "t_test = np.eye(10)[t_test].astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 2.31, acc: 0.114\n",
      "epoch: 2, loss: 2.3, acc: 0.118\n",
      "epoch: 3, loss: 2.3, acc: 0.123\n",
      "epoch: 4, loss: 2.29, acc: 0.129\n",
      "epoch: 5, loss: 2.29, acc: 0.136\n",
      "epoch: 6, loss: 2.29, acc: 0.147\n",
      "epoch: 7, loss: 2.28, acc: 0.160\n",
      "epoch: 8, loss: 2.28, acc: 0.177\n",
      "epoch: 9, loss: 2.27, acc: 0.194\n",
      "epoch: 10, loss: 2.26, acc: 0.212\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\4\\04_mnist.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=29'>30</a>\u001b[0m     start \u001b[39m=\u001b[39m batch \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=30'>31</a>\u001b[0m     end \u001b[39m=\u001b[39m start \u001b[39m+\u001b[39m batch_size\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=31'>32</a>\u001b[0m     train_step(x_[start:end], t_[start:end])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{:.3}\u001b[39;00m\u001b[39m, acc: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=34'>35</a>\u001b[0m     epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=35'>36</a>\u001b[0m     train_loss\u001b[39m.\u001b[39mresult(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=36'>37</a>\u001b[0m     train_acc\u001b[39m.\u001b[39mresult()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=37'>38</a>\u001b[0m ))\n",
      "\u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\4\\04_mnist.ipynb Cell 9'\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x, t)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=12'>13</a>\u001b[0m     preds \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=13'>14</a>\u001b[0m     loss \u001b[39m=\u001b[39m compute_loss(t, preds)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=14'>15</a>\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, model\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, model\u001b[39m.\u001b[39mtrainable_variables))\n",
      "\u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\4\\04_mnist.ipynb Cell 9'\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(t, y)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_loss\u001b[39m(t, y):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/open/deeplearning-keras-tf2-torch/4/04_mnist.ipynb#ch0000008?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m criterion(t, y)\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\keras\\losses.py:140\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    138\u001b[0m   call_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx())\n\u001b[0;32m    139\u001b[0m losses \u001b[39m=\u001b[39m call_fn(y_true, y_pred)\n\u001b[1;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m losses_utils\u001b[39m.\u001b[39;49mcompute_weighted_loss(\n\u001b[0;32m    141\u001b[0m     losses, sample_weight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_reduction())\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\keras\\utils\\losses_utils.py:331\u001b[0m, in \u001b[0;36mcompute_weighted_loss\u001b[1;34m(losses, sample_weight, reduction, name)\u001b[0m\n\u001b[0;32m    328\u001b[0m weighted_losses \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmultiply(losses, sample_weight)\n\u001b[0;32m    330\u001b[0m \u001b[39m# Apply reduction function to the individual weighted losses.\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m loss \u001b[39m=\u001b[39m reduce_weighted_loss(weighted_losses, reduction)\n\u001b[0;32m    332\u001b[0m \u001b[39mif\u001b[39;00m input_casted:\n\u001b[0;32m    333\u001b[0m   \u001b[39m# Convert the result back to the input type.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m   loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(loss, input_dtype)\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\keras\\utils\\losses_utils.py:269\u001b[0m, in \u001b[0;36mreduce_weighted_loss\u001b[1;34m(weighted_losses, reduction)\u001b[0m\n\u001b[0;32m    267\u001b[0m   loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(weighted_losses)\n\u001b[0;32m    268\u001b[0m   \u001b[39mif\u001b[39;00m reduction \u001b[39m==\u001b[39m ReductionV2\u001b[39m.\u001b[39mSUM_OVER_BATCH_SIZE:\n\u001b[1;32m--> 269\u001b[0m     loss \u001b[39m=\u001b[39m _safe_mean(loss, _num_elements(weighted_losses))\n\u001b[0;32m    270\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\keras\\utils\\losses_utils.py:251\u001b[0m, in \u001b[0;36m_safe_mean\u001b[1;34m(losses, num_present)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_mean\u001b[39m(losses, num_present):\n\u001b[0;32m    241\u001b[0m   \u001b[39m\"\"\"Computes a safe mean of the losses.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \n\u001b[0;32m    243\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39m      then zero is returned.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m   total_loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreduce_sum(losses)\n\u001b[0;32m    252\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mdivide_no_nan(total_loss, num_present, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2312\u001b[0m, in \u001b[0;36mreduce_sum\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2248\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmath.reduce_sum\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreduce_sum\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m   2249\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m   2250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreduce_sum\u001b[39m(input_tensor, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   2251\u001b[0m   \u001b[39m\"\"\"Computes the sum of elements across dimensions of a tensor.\u001b[39;00m\n\u001b[0;32m   2252\u001b[0m \n\u001b[0;32m   2253\u001b[0m \u001b[39m  This is the reduction operation for the elementwise `tf.math.add` op.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2308\u001b[0m \u001b[39m  @end_compatibility\u001b[39;00m\n\u001b[0;32m   2309\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m   2311\u001b[0m   \u001b[39mreturn\u001b[39;00m reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[1;32m-> 2312\u001b[0m                               _ReductionDims(input_tensor, axis))\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2152\u001b[0m, in \u001b[0;36m_ReductionDims\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   2149\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39mconstant(np\u001b[39m.\u001b[39marange(x_rank, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32))\n\u001b[0;32m   2150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2151\u001b[0m   \u001b[39m# Otherwise, we rely on Range and Rank to do the right thing at run-time.\u001b[39;00m\n\u001b[1;32m-> 2152\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m, array_ops\u001b[39m.\u001b[39;49mrank(x))\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2123\u001b[0m, in \u001b[0;36mrange\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   2120\u001b[0m limit \u001b[39m=\u001b[39m cast(limit, inferred_dtype)\n\u001b[0;32m   2121\u001b[0m delta \u001b[39m=\u001b[39m cast(delta, inferred_dtype)\n\u001b[1;32m-> 2123\u001b[0m \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49m_range(start, limit, delta, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\open\\deeplearning-keras-tf2-torch\\dl-tf-ptvenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:7732\u001b[0m, in \u001b[0;36m_range\u001b[1;34m(start, limit, delta, name)\u001b[0m\n\u001b[0;32m   7730\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   7731\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 7732\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   7733\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mRange\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, start, limit, delta)\n\u001b[0;32m   7734\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   7735\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = DNN(200, 10)\n",
    "\n",
    "criterion = losses.CategoricalCrossentropy()\n",
    "optimizer = optimizers.SGD(learning_rate=0.01)\n",
    "train_loss = metrics.Mean()\n",
    "train_acc = metrics.CategoricalAccuracy()\n",
    "\n",
    "def compute_loss(t, y):\n",
    "    return criterion(t, y)\n",
    "\n",
    "def train_step(x, t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x)\n",
    "        loss = compute_loss(t, preds)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_acc(t, preds)\n",
    "\n",
    "    return loss\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "n_batches = x_train.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    x_, t_ = shuffle(x_train, t_train)\n",
    "\n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        train_step(x_[start:end], t_[start:end])\n",
    "\n",
    "    print('epoch: {}, loss: {:.3}, acc: {:.3f}'.format(\n",
    "        epoch+1,\n",
    "        train_loss.result(),\n",
    "        train_acc.result()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz to C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz to C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:\\Users\\hp\\.torch\\mnist\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "root = os.path.join('~', '.torch', 'mnist')\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                lambda x: x.view(-1)])\n",
    "mnist_train = datasets.MNIST(root=root,\n",
    "                                download=True,\n",
    "                                train=True,\n",
    "                                transform=transform)\n",
    "mnist_test = datasets.MNIST(root=root,\n",
    "                            download=True,\n",
    "                            train=False,\n",
    "                            transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(mnist_train,\n",
    "                                batch_size=100,\n",
    "                                shuffle=True)\n",
    "test_dataloader = DataLoader(mnist_test,\n",
    "                                batch_size=100,\n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.a1 = nn.Sigmoid()\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a2 = nn.Sigmoid()\n",
    "        self.l3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a3 = nn.Sigmoid()\n",
    "        self.l4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.layers = [self.l1, self.a1,\n",
    "                       self.l2, self.a2,\n",
    "                       self.l3, self.a3,\n",
    "                       self.l4]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.a1 = nn.Tanh()\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a2 = nn.Tanh()\n",
    "        self.l3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a3 = nn.Tanh()\n",
    "        self.l4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.layers = [self.l1, self.a1,\n",
    "                       self.l2, self.a2,\n",
    "                       self.l3, self.a3,\n",
    "                       self.l4]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.82, acc: 0.533\n",
      "epoch: 2, loss: 0.757, acc: 0.799\n",
      "epoch: 3, loss: 0.492, acc: 0.865\n",
      "epoch: 4, loss: 0.408, acc: 0.886\n",
      "epoch: 5, loss: 0.368, acc: 0.897\n",
      "epoch: 6, loss: 0.343, acc: 0.903\n",
      "epoch: 7, loss: 0.325, acc: 0.907\n",
      "epoch: 8, loss: 0.31, acc: 0.911\n",
      "epoch: 9, loss: 0.298, acc: 0.915\n",
      "epoch: 10, loss: 0.287, acc: 0.918\n",
      "epoch: 11, loss: 0.277, acc: 0.920\n",
      "epoch: 12, loss: 0.268, acc: 0.923\n",
      "epoch: 13, loss: 0.259, acc: 0.925\n",
      "epoch: 14, loss: 0.252, acc: 0.928\n",
      "epoch: 15, loss: 0.244, acc: 0.930\n",
      "epoch: 16, loss: 0.237, acc: 0.932\n",
      "epoch: 17, loss: 0.23, acc: 0.934\n",
      "epoch: 18, loss: 0.223, acc: 0.936\n",
      "epoch: 19, loss: 0.217, acc: 0.937\n",
      "epoch: 20, loss: 0.21, acc: 0.940\n",
      "epoch: 21, loss: 0.204, acc: 0.942\n",
      "epoch: 22, loss: 0.198, acc: 0.943\n",
      "epoch: 23, loss: 0.192, acc: 0.945\n",
      "epoch: 24, loss: 0.187, acc: 0.946\n",
      "epoch: 25, loss: 0.181, acc: 0.948\n",
      "epoch: 26, loss: 0.176, acc: 0.950\n",
      "epoch: 27, loss: 0.171, acc: 0.951\n",
      "epoch: 28, loss: 0.166, acc: 0.953\n",
      "epoch: 29, loss: 0.161, acc: 0.954\n",
      "epoch: 30, loss: 0.157, acc: 0.955\n"
     ]
    }
   ],
   "source": [
    "model = DNN(784, 200, 10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizers.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def compute_loss(t, y):\n",
    "    return criterion(y, t)\n",
    "\n",
    "def train_step(x, t):\n",
    "    model.train()\n",
    "    preds = model(x)\n",
    "    loss = compute_loss(t, preds)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, preds\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "\n",
    "    for (x, t) in train_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = train_step(x, t)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += \\\n",
    "            accuracy_score(t.tolist(),\n",
    "                            preds.argmax(dim=-1).tolist())\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "\n",
    "    print('epoch: {}, loss: {:.3}, acc: {:.3f}'.format(\n",
    "        epoch+1,\n",
    "        train_loss,\n",
    "        train_acc\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.161, test_acc: 0.953\n"
     ]
    }
   ],
   "source": [
    "def test_step(x, t):\n",
    "    model.eval()\n",
    "    preds = model(x)\n",
    "    loss = criterion(preds, t)\n",
    "\n",
    "    return loss, preds\n",
    "\n",
    "test_loss = 0.\n",
    "test_acc = 0.\n",
    "\n",
    "for (x, t) in test_dataloader:\n",
    "    x, t = x.to(device), t.to(device)\n",
    "    loss, preds = test_step(x, t)\n",
    "    test_loss += loss.item()\n",
    "    test_acc += \\\n",
    "        accuracy_score(t.tolist(),\n",
    "                        preds.argmax(dim=-1).tolist())\n",
    "\n",
    "test_loss /= len(test_dataloader)\n",
    "test_acc /= len(test_dataloader)\n",
    "print('test_loss: {:.3f}, test_acc: {:.3f}'.format(\n",
    "    test_loss,\n",
    "    test_acc\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('dl-tf-ptvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db57a9e83bb1ac205d6811bc29cda304eb81c335675697ff0a5eeb5067f7012c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
